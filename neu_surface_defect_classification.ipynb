{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# NEU Surface Defect Image Classification\n\nThis notebook demonstrates how to train and evaluate an image classification model using the NEU surface defect dataset that is distributed in YOLO format. The workflow covers downloading the dataset, converting it into a folder layout that `torchvision` can consume, training a transfer learning model, and running inference on held-out images."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Environment setup\nInstall the required dependencies for PyTorch, data processing, and evaluation."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# If you are running this notebook on a CPU-only machine you can install the CPU wheels from PyTorch.\n# Remove the index override below if you have a CUDA-enabled environment configured.\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n!pip install -q pyyaml scikit-learn\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Imports and configuration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os\nimport random\nimport shutil\nimport zipfile\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport yaml\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom torch import nn\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, models, transforms\n\nplt.rcParams['figure.figsize'] = (8, 6)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint(f\"Using device: {DEVICE}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Download the NEU YOLO dataset\n\nThe following cell downloads the dataset archive from Kaggle using the provided curl command. Make sure your Kaggle API credentials are configured in the environment before running this step."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "dataset_dir = Path(\"data/neu_yolo\")\ndataset_dir.mkdir(parents=True, exist_ok=True)\nzip_path = dataset_dir / \"neu-yolo.zip\"\n\nif not zip_path.exists():\n    print(\"Downloading dataset...\")\n    !curl -L -o ./data/neu_yolo/neu-yolo.zip   https://www.kaggle.com/api/v1/datasets/download/zymzym/neu-yolo\nelse:\n    print(\"Dataset archive already exists, skipping download.\")\n\nprint(f\"Archive location: {zip_path}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Extract and reorganize data for classification\n\nThe dataset ships in YOLO format with separate image and label folders. The helper function below extracts the archive and converts it into a standard folder layout (`split/class_name/image.jpg`) that can be consumed by `torchvision.datasets.ImageFolder`. Each image is assigned the class that appears in its corresponding YOLO label file."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "raw_root = dataset_dir / \"raw\"\nclassification_root = Path(\"data/neu_classification\")\n\n\ndef extract_archive(zip_file: Path, target_dir: Path) -> Path:\n    if target_dir.exists() and any(target_dir.iterdir()):\n        print(f\"Using existing extracted contents in {target_dir}\")\n        return target_dir\n\n    print(f\"Extracting {zip_file} to {target_dir} ...\")\n    target_dir.mkdir(parents=True, exist_ok=True)\n    with zipfile.ZipFile(zip_file) as zf:\n        zf.extractall(target_dir)\n    print(\"Extraction complete.\")\n    return target_dir\n\n\ndef read_class_names(dataset_root: Path):\n    yaml_path = next(dataset_root.rglob(\"data.yaml\"), None)\n    if yaml_path is None:\n        raise FileNotFoundError(\"Could not locate data.yaml inside the extracted dataset.\")\n\n    with open(yaml_path, \"r\") as f:\n        data_cfg = yaml.safe_load(f)\n\n    names = data_cfg.get(\"names\") or data_cfg.get(\"class_names\")\n    if isinstance(names, dict):\n        names = [names[k] for k in sorted(names, key=lambda x: int(x) if isinstance(x, str) and x.isdigit() else x)]\n    if not isinstance(names, (list, tuple)):\n        raise ValueError(\"Unable to parse class names from data.yaml\")\n    return list(names)\n\n\ndef convert_to_classification_layout(dataset_root: Path, target_root: Path) -> None:\n    target_root.mkdir(parents=True, exist_ok=True)\n    class_names = read_class_names(dataset_root)\n    print(f\"Detected classes: {class_names}\")\n\n    for split in [\"train\", \"valid\", \"test\"]:\n        image_dir = dataset_root / split / \"images\"\n        label_dir = dataset_root / split / \"labels\"\n        if not image_dir.exists():\n            print(f\"Skipping missing split: {split}\")\n            continue\n\n        for label_file in sorted(label_dir.glob(\"*.txt\")):\n            with open(label_file, \"r\") as f:\n                first_line = f.readline().strip()\n            if not first_line:\n                continue\n            class_idx = int(first_line.split()[0])\n            class_name = class_names[class_idx]\n\n            candidate = image_dir / f\"{label_file.stem}.jpg\"\n            if not candidate.exists():\n                candidate = image_dir / f\"{label_file.stem}.png\"\n            if not candidate.exists():\n                matches = list(image_dir.glob(f\"{label_file.stem}.*\"))\n                if not matches:\n                    continue\n                candidate = matches[0]\n\n            dest_dir = target_root / split / class_name\n            dest_dir.mkdir(parents=True, exist_ok=True)\n            shutil.copy(candidate, dest_dir / candidate.name)\n\n    print(f\"Conversion complete. Classification data stored in {target_root}\")\n\n\nextract_archive(zip_path, raw_root)\nconvert_to_classification_layout(raw_root, classification_root)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Create datasets and data loaders\nWe apply data augmentation to the training split and create `DataLoader` objects for efficient batching."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "image_size = 224\nbatch_size = 32\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ninference_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dir = classification_root / \"train\"\nvalid_dir = classification_root / \"valid\"\ntest_dir = classification_root / \"test\"\n\nif not train_dir.exists():\n    raise RuntimeError(\"Training split was not created. Please verify the dataset conversion step.\")\n\nbase_train_dataset = datasets.ImageFolder(train_dir)\nclass_names = base_train_dataset.classes\nnum_classes = len(class_names)\n\nval_ratio = 0.2 if len(base_train_dataset) > 5 else 0.0\nif valid_dir.exists():\n    valid_dataset = datasets.ImageFolder(valid_dir, transform=inference_transform)\n    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\nelse:\n    indices = torch.randperm(len(base_train_dataset), generator=torch.Generator().manual_seed(SEED)).tolist()\n    val_size = int(len(base_train_dataset) * val_ratio)\n    val_indices = indices[:val_size]\n    train_indices = indices[val_size:]\n    train_dataset = Subset(datasets.ImageFolder(train_dir, transform=train_transform), train_indices)\n    valid_dataset = Subset(datasets.ImageFolder(train_dir, transform=inference_transform), val_indices) if val_indices else None\n\nif test_dir.exists():\n    test_dataset = datasets.ImageFolder(test_dir, transform=inference_transform)\nelse:\n    test_dataset = valid_dataset\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True) if valid_dataset is not None else None\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True) if test_dataset is not None else None\n\nprint(f\"Classes: {class_names}\")\nprint(f\"Train batches: {len(train_loader)}\")\nif valid_loader:\n    print(f\"Validation batches: {len(valid_loader)}\")\nif test_loader:\n    print(f\"Test batches: {len(test_loader)}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Preview a batch of training images"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def show_batch(loader):\n    images, labels = next(iter(loader))\n    unnorm = images.clone()\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n    unnorm = unnorm * std + mean\n\n    grid_rows = 4\n    grid_cols = 4\n    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(8, 8))\n    for ax, img, label in zip(axes.flatten(), unnorm[: grid_rows * grid_cols], labels[: grid_rows * grid_cols]):\n        img = img.permute(1, 2, 0).cpu().numpy()\n        img = np.clip(img, 0, 1)\n        ax.imshow(img)\n        ax.set_title(class_names[label])\n        ax.axis(\"off\")\n    plt.tight_layout()\n\nshow_batch(train_loader)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Define the model and training utilities\nWe fine-tune a pretrained ResNet-18 model from `torchvision` by replacing its final classification layer."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "weights = models.ResNet18_Weights.DEFAULT\nmodel = models.resnet18(weights=weights)\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\nmodel = model.to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\nprint(model)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model, loader, criterion, device):\n    if loader is None:\n        return None, None\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Train the model"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "num_epochs = 5\nbest_valid_acc = 0.0\nbest_state = None\n\nfor epoch in range(1, num_epochs + 1):\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n    valid_loss, valid_acc = evaluate(model, valid_loader, criterion, DEVICE)\n    scheduler.step()\n\n    msg = f\"Epoch {epoch}/{num_epochs} - Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}\"\n    if valid_loader:\n        msg += f\" | Valid loss: {valid_loss:.4f}, Valid acc: {valid_acc:.4f}\"\n        if valid_acc > best_valid_acc:\n            best_valid_acc = valid_acc\n            best_state = model.state_dict()\n    print(msg)\n\nif best_state is not None:\n    model.load_state_dict(best_state)\n    print(f\"Loaded best model weights with validation accuracy {best_valid_acc:.4f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Evaluate on the test split"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if test_loader is None:\n    raise RuntimeError(\"Test loader is not available. Ensure the dataset contains a test or validation split.\")\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(DEVICE)\n        outputs = model(images)\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels.numpy())\n\nprint(classification_report(all_labels, all_preds, target_names=class_names))\nprint(\"Confusion matrix:\n\", confusion_matrix(all_labels, all_preds))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Run inference on a single image"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nimport PIL.Image as Image\n\ndef predict_image(image_path: Path):\n    image = Image.open(image_path).convert(\"RGB\")\n    input_tensor = inference_transform(image).unsqueeze(0).to(DEVICE)\n    model.eval()\n    with torch.no_grad():\n        output = model(input_tensor)\n        probs = torch.nn.functional.softmax(output, dim=1)[0].cpu().numpy()\n    top_idx = np.argmax(probs)\n    return class_names[top_idx], probs[top_idx], probs\n\n# Pick a random image from the test set (or validation set if test is not available)\nsearch_dir = test_dir if test_dir.exists() else train_dir\nextensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\")\ncandidates = []\nfor pattern in extensions:\n    candidates.extend(search_dir.rglob(pattern))\nif not candidates:\n    raise RuntimeError(f\"No images found in {search_dir}.\")\nrandom_image = random.choice(candidates)\nlabel, confidence, probas = predict_image(random_image)\n\nprint(f\"Predicted class: {label} (confidence: {confidence:.2%})\")\nprint(f\"Image path: {random_image}\")\n\nplt.imshow(Image.open(random_image))\nplt.title(f\"Predicted: {label} ({confidence:.1%})\")\nplt.axis('off')\nplt.show()\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}